model:
  dim: 128
  layers: 4
  heads: 4
  vocab_size: 100
  seq_len: 128

training:
  lr: 0.0002
  batch_size: 64
  epochs: 5
